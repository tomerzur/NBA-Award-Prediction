{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Model<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import rbo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will be using a multi-layer perceptron to make predictions for each award. This notebook includes the code I used to create this neural model and make predictions for each award.\n",
    "<br><br>\n",
    "This code should be run after you have finished running the 'preprocessing' notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Helper Functions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am loading helper functions that I wrote in order to load my data, scale the data, fill in missing/NA values, and calculate and print my accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_TEST_RESULTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves train, dev, and test data for the specified award\n",
    "def get_data(award_name):\n",
    "    x_train_pnames = pd.read_csv(f'data/train_x_{award}.csv', index_col=0)\n",
    "    y_train = pd.read_csv(f'data/train_y_{award}.csv', index_col=0)\n",
    "    x_dev_pnames = pd.read_csv(f'data/dev_x_{award}.csv', index_col=0)\n",
    "    y_dev = pd.read_csv(f'data/dev_y_{award}.csv', index_col=0)\n",
    "    x_test_pnames = pd.read_csv(f'data/test_x_{award}.csv', index_col=0)\n",
    "    y_test = pd.read_csv(f'data/test_y_{award}.csv', index_col=0)\n",
    "    x_train = x_train_pnames.drop(columns=['player', 'season'])\n",
    "    x_dev = x_dev_pnames.drop(columns=['player', 'season'])\n",
    "    x_test = x_test_pnames.drop(columns=['player', 'season'])\n",
    "    return x_train_pnames, x_train, y_train, x_dev_pnames, x_dev, y_dev, x_test_pnames, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some missing statistics in my dataset. These occur mostly for older players, as there are some statistics (such as steals and blocks) that weren't kept track of when the NBA first started. I will deal with these missing statistics in three ways:\n",
    "<br>\n",
    "Method 2 - Fill in zeros for all of the missing stats, and use all the data\n",
    "<br>\n",
    "Method 5 - Fill in mean values for all of missing stats, and use all of the data\n",
    "<br>\n",
    "Method 4 - Use linear regressions to predict each of the missing stats, and use all of the data\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Some other ways I may implement in the future to explore dealing with missing statistics are:\n",
    "<br>\n",
    "Remove all of these rows (This should end up using all player seasons after 1979)\n",
    "<br>\n",
    "Drop all the columns with missing stats, and use all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the missing statistics (that were not kept track of for older players)\n",
    "# using one of the three methods mentioned above\n",
    "def fill_missing_stats(x_train, x_train_pnames, x_dev, x_dev_pnames, x_test, x_test_pnames, method=4):\n",
    "    if method == 2:\n",
    "        x_train_filled = x_train.fillna(0)\n",
    "        x_dev_filled = x_dev.fillna(0)\n",
    "        x_test_filled = x_test.fillna(0)\n",
    "    elif method == 4:\n",
    "        x_train_filled = copy.copy(x_train_pnames)\n",
    "        x_dev_filled = copy.copy(x_dev_pnames)\n",
    "        x_test_filled = copy.copy(x_test_pnames)\n",
    "        # three_p and three_pa - if season < 1980, set to NA\n",
    "        x_train_filled['three_p'] = np.where(x_train_filled.season < 1980, float('NaN'), x_train_filled.three_p)\n",
    "        x_train_filled['three_pa'] = np.where(x_train_filled.season < 1980, float('NaN'), x_train_filled.three_pa)\n",
    "        \n",
    "        x_train_filled = x_train_filled.drop(columns=['player', 'season'])\n",
    "        x_dev_filled = x_dev_filled.drop(columns=['player', 'season'])\n",
    "        x_test_filled = x_test_filled.drop(columns=['player', 'season'])\n",
    "\n",
    "        # predict all missing values for these stats using lin reg\n",
    "        # train data: player seasons where stat != NaN\n",
    "        # test data: player seasons where stat == NaN\n",
    "        for stat in ['three_p', 'three_pa', 'gs', 'orb', 'drb', 'stl', 'blk', 'tov']:\n",
    "            # check if this stat has missing values\n",
    "            if x_train_filled[stat].isnull().values.any():\n",
    "                train_stat = x_train_filled[x_train_filled[stat].notna()]\n",
    "                # for all lin reg predictions, don't use any of the columns that have missing values\n",
    "                x_train_stat = train_stat.drop(columns=['three_p', 'three_pa', 'gs', 'orb', 'drb', 'stl', 'blk', 'tov', 'three_pct', 'gs_pct'])\n",
    "                y_train_stat = train_stat[[stat]]\n",
    "                test_stat = x_train_filled[~x_train_filled[stat].notna()]\n",
    "                x_test_stat = test_stat.drop(columns=['three_p', 'three_pa', 'gs', 'orb', 'drb', 'stl', 'blk', 'tov', 'three_pct', 'gs_pct'])\n",
    "\n",
    "                reg_stat = LinearRegression().fit(x_train_stat, y_train_stat)\n",
    "                pred_test_stat = reg_stat.predict(x_test_stat)\n",
    "                pred_stat_dict = {index: round(value[0], 3) for index, value in zip(x_train_filled[~x_train_filled[stat].notna()].index, pred_test_stat)}\n",
    "                x_train_filled[stat] = x_train_filled[stat].fillna(pred_stat_dict)\n",
    "\n",
    "        # fill in three_pct - three_p / three_pa, gs_pct - gs / g\n",
    "        x_train_filled['three_pct'] = x_train_filled['three_p'] / x_train_filled['three_pa']\n",
    "        x_train_filled['gs_pct'] = x_train_filled['gs'] / x_train_filled['g']\n",
    "        x_train_filled = x_train_filled.fillna(0)\n",
    "        x_dev_filled['three_pct'] = x_dev_filled['three_p'] / x_dev_filled['three_pa']\n",
    "        x_dev_filled = x_dev_filled.fillna(0)\n",
    "        x_test_filled['three_pct'] = x_test_filled['three_p'] / x_test_filled['three_pa']\n",
    "        x_test_filled = x_test_filled.fillna(0)\n",
    "    elif method == 5:\n",
    "        x_train_filled = x_train.fillna(x_train.mean())\n",
    "        x_dev_filled = x_dev.fillna(x_dev.mean())\n",
    "        x_test_filled = x_test.fillna(x_test.mean())\n",
    "    else:\n",
    "        print('method of dealing with missing stats must be either 2, 4, or 5')\n",
    "    return x_train_filled, x_dev_filled, x_test_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The award points that players receive are very skewed, as most players received zero points for a given award. To reduce this skew, I scaled all of the award point values logarithmically.\n",
    "<br>\n",
    "<br>\n",
    "I also applied a Min Max scaler so that each feature would be in the range (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log scale all the y values, then min-max scale all the x and y values\n",
    "def scale_vals(x_vals, y_vals):\n",
    "    # add 1 so that you can take log of players with 0 award points\n",
    "    y_log_vals = np.log(y_vals.award_pts_won + 1).values.reshape(-1, 1)\n",
    "    x_scaler = MinMaxScaler().fit(x_vals, y_vals)\n",
    "    x_vals = x_scaler.transform(x_vals)\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_vals = y_scaler.fit_transform(y_log_vals)\n",
    "    return x_vals, y_vals, y_scaler\n",
    "    \n",
    "def unscale_vals(y_vals_scaled, y_scaler):\n",
    "    y_log_vals = y_scaler.inverse_transform(y_vals_scaled)\n",
    "    y_vals = np.expm1(y_log_vals)\n",
    "    return y_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to measure my models' performance using three metrics: Mean Squared Error, % of correct MVP predictions, and Rank-Biased Overlap.\n",
    "<br>\n",
    "I chose to use rank-biased overlap because it is an accuracy metric for rankings that weights higher ranked items more than lower ranked items. In addition, when comparing two lists using this metric, rank-biased overlap can deal with items that occur in one list but are not seen in the other list.\n",
    "<br>\n",
    "For more information on rank-biased overlap, see this article: http://codalism.com/research/papers/wmz10_tois.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print accuracy metrics by comparing the given lists (y_pred and y_actual)\n",
    "# accuracy metrics that I am using are % of correct winners picked, rank biased overlap, and mean squared error\n",
    "def print_accuracy(y_pred, y_actual, x_pnames, rbo_cutoff = None, verbose=0):\n",
    "    all_data = copy.copy(x_pnames)\n",
    "    all_data['award_pts_actual'] = y_actual['award_pts_won']\n",
    "    all_data['award_pts_pred'] = y_pred\n",
    "    num_correct = 0\n",
    "    num_yrs = 0\n",
    "    rbo_vals = []\n",
    "    for year in set(all_data.season):\n",
    "        # a. % of correct winners picked\n",
    "        data_in_yr = all_data[all_data['season'] == year]\n",
    "        pred_winner_row = data_in_yr['award_pts_pred'].argmax()\n",
    "        actual_winner_row = data_in_yr['award_pts_actual'].argmax()\n",
    "        pred_winner, pred_pts = data_in_yr.iloc[pred_winner_row]['player'], data_in_yr.iloc[pred_winner_row]['award_pts_pred']\n",
    "        actual_winner, actual_pts = data_in_yr.iloc[actual_winner_row]['player'], data_in_yr.iloc[actual_winner_row]['award_pts_actual']\n",
    "        if verbose > 0:\n",
    "            print(f'{year}')\n",
    "            print(f'Predicted Winner: {pred_winner} ({pred_pts} award pts)')\n",
    "            print(f'Actual Winner: {actual_winner} ({actual_pts} award pts)')\n",
    "        if pred_winner == actual_winner:\n",
    "            num_correct += 1\n",
    "        num_yrs += 1\n",
    "        \n",
    "        # b. Rank-Biased Overlap\n",
    "        # calculate RBO:\n",
    "        # get rows in given year with players that received votes - sorted by num votes\n",
    "        vote_getters_df = data_in_yr[data_in_yr['award_pts_actual'] > 0]\n",
    "        num_vote_getters = len(vote_getters_df)\n",
    "        vote_getters_df = vote_getters_df.sort_values(by=['award_pts_actual'], ascending=False)\n",
    "        # get top-(num_vote_getters) rows from predictions\n",
    "        pred_vote_getters_df = data_in_yr.sort_values(by=['award_pts_pred'], ascending=False)\n",
    "        if rbo_cutoff == None:\n",
    "            pred_vote_getters_df = pred_vote_getters_df[:num_vote_getters]\n",
    "        else:\n",
    "            cutoff = min(rbo_cutoff, num_vote_getters)\n",
    "            vote_getters_df = vote_getters_df[:cutoff]\n",
    "            pred_vote_getters_df = pred_vote_getters_df[:cutoff]\n",
    "        vote_getters = vote_getters_df['player'].values\n",
    "        pred_vote_getters = pred_vote_getters_df['player'].values\n",
    "        # deal with edge case where two vote getters have exact same name\n",
    "        vote_getters = list(dict.fromkeys(vote_getters))\n",
    "        pred_vote_getters = list(dict.fromkeys(pred_vote_getters))\n",
    "        #print(len(vote_getters))\n",
    "        #print(len(pred_vote_getters))\n",
    "        if verbose > 1:\n",
    "            print('Actual vote getters:')\n",
    "            print(vote_getters)\n",
    "            print(f'Predicted top-{num_vote_getters} vote getters:')\n",
    "            print(pred_vote_getters)\n",
    "        # compute RBO from these two lists\n",
    "        rbo_num = rbo.RankingSimilarity(vote_getters, pred_vote_getters).rbo()\n",
    "        rbo_vals.append(rbo_num)\n",
    "        if verbose > 0:\n",
    "            print(f'Rank Biased Overlap: {rbo_num}')\n",
    "        \n",
    "    print(f'% of winners predicted correctly: {round(num_correct / num_yrs * 100, 2)}%')\n",
    "    print(f'Average Rank-Biased Overlap: {round(sum(rbo_vals) / len(rbo_vals), 3)}')\n",
    "    # c. MSE\n",
    "    mse = mean_squared_error(y_actual, y_pred)\n",
    "    print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implementing Neural Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I implement a separate neural model for predicting each of the five awards. I manually tested changing a bunch of different hyperparameters one at a time in order to come up with the best set of hyperparameters I could (I have commented out a lot of layers/hyperparameters that I tested and didn't end up using, since they did not positively impact the model's accuracy). One area for future improvement would be to use grid search or another method to search for better hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*****mvp*****\n",
      "\n",
      "filling na values using method 4\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 40.0%\n",
      "Average Rank-Biased Overlap: 0.568\n",
      "Mean Squared Error: 3454.9779777922013\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 20.0%\n",
      "Average Rank-Biased Overlap: 0.491\n",
      "Mean Squared Error: 5248.868856178764\n",
      "\n",
      "filling na values using method 2\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 40.0%\n",
      "Average Rank-Biased Overlap: 0.563\n",
      "Mean Squared Error: 3454.991059468207\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 20.0%\n",
      "Average Rank-Biased Overlap: 0.543\n",
      "Mean Squared Error: 5248.881431492934\n",
      "\n",
      "filling na values using method 5\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 34.55%\n",
      "Average Rank-Biased Overlap: 0.56\n",
      "Mean Squared Error: 3455.007525074799\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 20.0%\n",
      "Average Rank-Biased Overlap: 0.492\n",
      "Mean Squared Error: 5248.900998974646\n",
      "\n",
      "\n",
      "*****dpoy*****\n",
      "\n",
      "filling na values using method 4\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 3.57%\n",
      "Average Rank-Biased Overlap: 0.259\n",
      "Mean Squared Error: 227.09035179778897\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 20.0%\n",
      "Average Rank-Biased Overlap: 0.234\n",
      "Mean Squared Error: 582.2359648812018\n",
      "\n",
      "filling na values using method 2\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 10.71%\n",
      "Average Rank-Biased Overlap: 0.278\n",
      "Mean Squared Error: 227.0956258587423\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 20.0%\n",
      "Average Rank-Biased Overlap: 0.237\n",
      "Mean Squared Error: 582.2498044517614\n",
      "\n",
      "filling na values using method 5\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 10.71%\n",
      "Average Rank-Biased Overlap: 0.267\n",
      "Mean Squared Error: 227.09449218070424\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 20.0%\n",
      "Average Rank-Biased Overlap: 0.248\n",
      "Mean Squared Error: 582.2468118702212\n",
      "\n",
      "\n",
      "*****roy*****\n",
      "\n",
      "filling na values using method 4\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 55.32%\n",
      "Average Rank-Biased Overlap: 0.722\n",
      "Mean Squared Error: 1366.1678578362087\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 80.0%\n",
      "Average Rank-Biased Overlap: 0.688\n",
      "Mean Squared Error: 6098.380270164807\n",
      "\n",
      "filling na values using method 2\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 55.32%\n",
      "Average Rank-Biased Overlap: 0.703\n",
      "Mean Squared Error: 1366.3021144845036\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 80.0%\n",
      "Average Rank-Biased Overlap: 0.696\n",
      "Mean Squared Error: 6098.8549811271505\n",
      "\n",
      "filling na values using method 5\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 55.32%\n",
      "Average Rank-Biased Overlap: 0.71\n",
      "Mean Squared Error: 1366.3272279650344\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 80.0%\n",
      "Average Rank-Biased Overlap: 0.696\n",
      "Mean Squared Error: 6098.944650624659\n",
      "\n",
      "\n",
      "*****mip*****\n",
      "\n",
      "filling na values using method 4\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 0.0%\n",
      "Average Rank-Biased Overlap: 0.176\n",
      "Mean Squared Error: 222.22625239928539\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 0.0%\n",
      "Average Rank-Biased Overlap: 0.23\n",
      "Mean Squared Error: 568.11295872229\n",
      "\n",
      "filling na values using method 2\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 0.0%\n",
      "Average Rank-Biased Overlap: 0.168\n",
      "Mean Squared Error: 222.22203382432974\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 0.0%\n",
      "Average Rank-Biased Overlap: 0.22\n",
      "Mean Squared Error: 568.1025395995147\n",
      "\n",
      "filling na values using method 5\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 0.0%\n",
      "Average Rank-Biased Overlap: 0.174\n",
      "Mean Squared Error: 222.22658850367452\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 0.0%\n",
      "Average Rank-Biased Overlap: 0.23\n",
      "Mean Squared Error: 568.1137875852124\n",
      "\n",
      "\n",
      "*****smoy*****\n",
      "\n",
      "filling na values using method 4\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 33.33%\n",
      "Average Rank-Biased Overlap: 0.472\n",
      "Mean Squared Error: 436.62313517206877\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 80.0%\n",
      "Average Rank-Biased Overlap: 0.551\n",
      "Mean Squared Error: 1240.9331032311711\n",
      "\n",
      "filling na values using method 2\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 33.33%\n",
      "Average Rank-Biased Overlap: 0.472\n",
      "Mean Squared Error: 436.6054505048063\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 80.0%\n",
      "Average Rank-Biased Overlap: 0.559\n",
      "Mean Squared Error: 1240.8902050173697\n",
      "\n",
      "filling na values using method 5\n",
      "\n",
      "Train accuracy:\n",
      "% of winners predicted correctly: 33.33%\n",
      "Average Rank-Biased Overlap: 0.466\n",
      "Mean Squared Error: 436.6271979693323\n",
      "\n",
      "Dev accuracy:\n",
      "% of winners predicted correctly: 40.0%\n",
      "Average Rank-Biased Overlap: 0.53\n",
      "Mean Squared Error: 1240.9429623771086\n"
     ]
    }
   ],
   "source": [
    "for award in ['mvp', 'dpoy', 'roy', 'mip', 'smoy']:\n",
    "    print(f'\\n\\n*****{award}*****')\n",
    "    x_train_pnames, x_train, y_train, x_dev_pnames, x_dev, y_dev, x_test_pnames, x_test, y_test = get_data(award)\n",
    "    for fill_na_method in [4, 2, 5]:\n",
    "        print(f'\\nfilling na values using method {fill_na_method}')\n",
    "        x_train_filled, x_dev_filled, x_test_filled = fill_missing_stats(x_train, x_train_pnames, x_dev, x_dev_pnames,\n",
    "                                                                         x_test, x_test_pnames, method=fill_na_method)\n",
    "        x_train_scaled, y_train_scaled, y_train_scaler = scale_vals(x_train_filled, y_train)\n",
    "        x_dev_scaled, y_dev_scaled, y_dev_scaler = scale_vals(x_dev_filled, y_dev)\n",
    "        \n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        #model.add(Dense(40, activation='relu', kernel_regularizer=l2(l=0.6)))\n",
    "        model.add(Dense(40, input_dim=x_train_filled.shape[1], activation='relu', kernel_regularizer=l2(l=0.6)))        \n",
    "        #model.add(Dense(40, activation='relu'))\n",
    "        #model.add(Dense(40, input_dim=x_train_filled.shape[1], activation='relu'))\n",
    "        #model.add(Dense(20, activation='relu', kernel_regularizer=l2(l=0.1)))\n",
    "        #model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(l=0.1)))\n",
    "        model.add(Dense(40, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        #model.add(Dense(1))\n",
    "        #model.compile(loss='mse', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
    "        model.compile(loss='mse', optimizer=SGD(lr=0.01, momentum=0.9, clipnorm=1.0), metrics=['accuracy'])\n",
    "        #model.compile(loss='mse', optimizer=SGD(lr=0.01, momentum=0.9), metrics=['accuracy'])\n",
    "        model.fit(x_train_scaled, y_train_scaled, epochs=50, batch_size=300, verbose=0)\n",
    "        \n",
    "        train_pred_scaled = model.predict(x_train_scaled)\n",
    "        dev_pred_scaled = model.predict(x_dev_scaled)\n",
    "        train_pred = unscale_vals(train_pred_scaled, y_train_scaler)\n",
    "        dev_pred = unscale_vals(dev_pred_scaled, y_dev_scaler)\n",
    "        print('\\nTrain accuracy:')\n",
    "        print_accuracy(train_pred, y_train, x_train_pnames)\n",
    "        print('\\nDev accuracy:')\n",
    "        print_accuracy(dev_pred, y_dev, x_dev_pnames, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Get Final Results</h2>\n",
    "After testing and tweaking my model (I have commented out a lot of layers/hyperparameters that I tested and didn't end up using, since they did not positively impact the model's accuracy), I calculated its final accuracy on my test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*****mvp*****\n",
      "\n",
      "filling na values using method 4\n",
      "\n",
      "Test accuracy:\n",
      "2016\n",
      "Predicted Winner: James Harden (0.1872488111257553 award pts)\n",
      "Actual Winner: Stephen Curry (1310.0 award pts)\n",
      "Rank Biased Overlap: 0.26996031746031746\n",
      "2017\n",
      "Predicted Winner: Russell Westbrook (0.18754832446575165 award pts)\n",
      "Actual Winner: Russell Westbrook (888.0 award pts)\n",
      "Rank Biased Overlap: 0.6150629673356947\n",
      "2018\n",
      "Predicted Winner: Russell Westbrook (0.18733541667461395 award pts)\n",
      "Actual Winner: James Harden (965.0 award pts)\n",
      "Rank Biased Overlap: 0.5177188196418965\n",
      "2019\n",
      "Predicted Winner: James Harden (0.1874047815799713 award pts)\n",
      "Actual Winner: Giannis Antetokounmpo (941.0 award pts)\n",
      "Rank Biased Overlap: 0.5094095719095718\n",
      "2020\n",
      "Predicted Winner: James Harden (0.1870918571949005 award pts)\n",
      "Actual Winner: Giannis Antetokounmpo (962.0 award pts)\n",
      "Rank Biased Overlap: 0.5765061327561327\n",
      "% of winners predicted correctly: 20.0%\n",
      "Average Rank-Biased Overlap: 0.498\n",
      "Mean Squared Error: 3766.1327961869806\n",
      "\n",
      "\n",
      "*****dpoy*****\n",
      "\n",
      "filling na values using method 4\n",
      "\n",
      "Test accuracy:\n",
      "2016\n",
      "Predicted Winner: Andre Drummond (0.10039030015468597 award pts)\n",
      "Actual Winner: Kawhi Leonard (547.0 award pts)\n",
      "Rank Biased Overlap: 0.1806059761077062\n",
      "2017\n",
      "Predicted Winner: Russell Westbrook (0.10037147253751755 award pts)\n",
      "Actual Winner: Draymond Green (434.0 award pts)\n",
      "Rank Biased Overlap: 0.05445326278659612\n",
      "2018\n",
      "Predicted Winner: Andre Drummond (0.10035782307386398 award pts)\n",
      "Actual Winner: Rudy Gobert (466.0 award pts)\n",
      "Rank Biased Overlap: 0.13492730837021494\n",
      "2019\n",
      "Predicted Winner: Andre Drummond (0.1003694087266922 award pts)\n",
      "Actual Winner: Rudy Gobert (411.0 award pts)\n",
      "Rank Biased Overlap: 0.1742857142857143\n",
      "2020\n",
      "Predicted Winner: James Harden (0.10029992461204529 award pts)\n",
      "Actual Winner: Giannis Antetokounmpo (432.0 award pts)\n",
      "Rank Biased Overlap: 0.47346079846079836\n",
      "% of winners predicted correctly: 0.0%\n",
      "Average Rank-Biased Overlap: 0.204\n",
      "Mean Squared Error: 616.9902951032782\n",
      "\n",
      "\n",
      "*****roy*****\n",
      "\n",
      "filling na values using method 4\n",
      "\n",
      "Test accuracy:\n",
      "2016\n",
      "Predicted Winner: Karl-Anthony Towns (0.2893890142440796 award pts)\n",
      "Actual Winner: Karl-Anthony Towns (650.0 award pts)\n",
      "Rank Biased Overlap: 0.7961640211640212\n",
      "2017\n",
      "Predicted Winner: Dario Šarić (0.28626227378845215 award pts)\n",
      "Actual Winner: Malcolm Brogdon (414.0 award pts)\n",
      "Rank Biased Overlap: 0.5264285714285715\n",
      "2018\n",
      "Predicted Winner: Ben Simmons (0.2889203429222107 award pts)\n",
      "Actual Winner: Ben Simmons (481.0 award pts)\n",
      "Rank Biased Overlap: 0.9333333333333332\n",
      "2019\n",
      "Predicted Winner: Trae Young (0.2886132597923279 award pts)\n",
      "Actual Winner: Luka Dončić (496.0 award pts)\n",
      "Rank Biased Overlap: 0.7383928571428571\n",
      "2020\n",
      "Predicted Winner: Ja Morant (0.2866643965244293 award pts)\n",
      "Actual Winner: Ja Morant (498.0 award pts)\n",
      "Rank Biased Overlap: 0.6578869047619047\n",
      "% of winners predicted correctly: 60.0%\n",
      "Average Rank-Biased Overlap: 0.73\n",
      "Mean Squared Error: 3685.202778969236\n",
      "\n",
      "\n",
      "*****mip*****\n",
      "\n",
      "filling na values using method 4\n",
      "\n",
      "Test accuracy:\n",
      "2016\n",
      "Predicted Winner: James Harden (0.13876840472221375 award pts)\n",
      "Actual Winner: CJ McCollum (559.0 award pts)\n",
      "Rank Biased Overlap: 0.2680494123179882\n",
      "2017\n",
      "Predicted Winner: Russell Westbrook (0.13886010646820068 award pts)\n",
      "Actual Winner: Giannis Antetokounmpo (428.0 award pts)\n",
      "Rank Biased Overlap: 0.19622619282424109\n",
      "2018\n",
      "Predicted Winner: Russell Westbrook (0.13873039186000824 award pts)\n",
      "Actual Winner: Victor Oladipo (499.0 award pts)\n",
      "Rank Biased Overlap: 0.1728452201317054\n",
      "2019\n",
      "Predicted Winner: James Harden (0.1387808620929718 award pts)\n",
      "Actual Winner: Pascal Siakam (469.0 award pts)\n",
      "Rank Biased Overlap: 0.15432908379889804\n",
      "2020\n",
      "Predicted Winner: James Harden (0.13852791488170624 award pts)\n",
      "Actual Winner: Brandon Ingram (326.0 award pts)\n",
      "Rank Biased Overlap: 0.2294882894882895\n",
      "% of winners predicted correctly: 0.0%\n",
      "Average Rank-Biased Overlap: 0.204\n",
      "Mean Squared Error: 660.1817964544693\n",
      "\n",
      "\n",
      "*****smoy*****\n",
      "\n",
      "filling na values using method 4\n",
      "\n",
      "Test accuracy:\n",
      "2016\n",
      "Predicted Winner: Will Barton (0.15985801815986633 award pts)\n",
      "Actual Winner: Jamal Crawford (341.0 award pts)\n",
      "Rank Biased Overlap: 0.46841665748297656\n",
      "2017\n",
      "Predicted Winner: Jordan Clarkson (0.15984578430652618 award pts)\n",
      "Actual Winner: Eric Gordon (358.0 award pts)\n",
      "Rank Biased Overlap: 0.37522797715105405\n",
      "2018\n",
      "Predicted Winner: Lou Williams (0.15996572375297546 award pts)\n",
      "Actual Winner: Lou Williams (495.0 award pts)\n",
      "Rank Biased Overlap: 0.4873498521854924\n",
      "2019\n",
      "Predicted Winner: Lou Williams (0.15988333523273468 award pts)\n",
      "Actual Winner: Lou Williams (489.0 award pts)\n",
      "Rank Biased Overlap: 0.5405417994703707\n",
      "2020\n",
      "Predicted Winner: Dennis Schröder (0.1597842127084732 award pts)\n",
      "Actual Winner: Montrezl Harrell (397.0 award pts)\n",
      "Rank Biased Overlap: 0.4115079365079365\n",
      "% of winners predicted correctly: 40.0%\n",
      "Average Rank-Biased Overlap: 0.457\n",
      "Mean Squared Error: 823.6375909055309\n"
     ]
    }
   ],
   "source": [
    "if GET_TEST_RESULTS:\n",
    "    for award in ['mvp', 'dpoy', 'roy', 'mip', 'smoy']:\n",
    "        print(f'\\n\\n*****{award}*****')\n",
    "        x_train_pnames, x_train, y_train, x_dev_pnames, x_dev, y_dev, x_test_pnames, x_test, y_test = get_data(award)\n",
    "        for fill_na_method in [4]:\n",
    "            print(f'\\nfilling na values using method {fill_na_method}')\n",
    "            x_train_filled, x_dev_filled, x_test_filled = fill_missing_stats(x_train, x_train_pnames, x_dev, x_dev_pnames,\n",
    "                                                                             x_test, x_test_pnames, method=fill_na_method)\n",
    "            x_train_scaled, y_train_scaled, y_train_scaler = scale_vals(x_train_filled, y_train)\n",
    "            x_dev_scaled, y_dev_scaled, y_dev_scaler = scale_vals(x_dev_filled, y_dev)\n",
    "            x_test_scaled, y_test_scaled, y_test_scaler = scale_vals(x_test_filled, y_test)\n",
    "\n",
    "            # create model\n",
    "            model = Sequential()\n",
    "            #model.add(Dense(40, activation='relu', kernel_regularizer=l2(l=0.6)))\n",
    "            model.add(Dense(40, input_dim=x_train_filled.shape[1], activation='relu', kernel_regularizer=l2(l=0.6)))        \n",
    "            #model.add(Dense(40, activation='relu'))\n",
    "            #model.add(Dense(40, input_dim=x_train_filled.shape[1], activation='relu'))\n",
    "            #model.add(Dense(20, activation='relu', kernel_regularizer=l2(l=0.1)))\n",
    "            #model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(l=0.1)))\n",
    "            model.add(Dense(40, activation='relu'))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            #model.add(Dense(1))\n",
    "            #model.compile(loss='mse', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
    "            model.compile(loss='mse', optimizer=SGD(lr=0.01, momentum=0.9, clipnorm=1.0), metrics=['accuracy'])\n",
    "            #model.compile(loss='mse', optimizer=SGD(lr=0.01, momentum=0.9), metrics=['accuracy'])\n",
    "            model.fit(x_train_scaled, y_train_scaled, epochs=50, batch_size=300, verbose=0)\n",
    "\n",
    "            train_pred_scaled = model.predict(x_train_scaled)\n",
    "            test_pred_scaled = model.predict(x_test_scaled)\n",
    "            train_pred = unscale_vals(train_pred_scaled, y_train_scaler)\n",
    "            test_pred = unscale_vals(test_pred_scaled, y_test_scaler)\n",
    "            print('\\nTest accuracy:')\n",
    "            print_accuracy(test_pred, y_test, x_test_pnames, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
